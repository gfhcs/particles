
#### NEUESTE GEDANKEN:

Man könnte ein Bezahlsystem hinzufügen: Nodes wählen dann immer den Nachbarn für einen Auftrag aus, der das beste Preis-Rechenzeit-Verhältnis anbietet. Bevor der Nachbar die Berechnung beginnt, muss über eine Bank ein wähnungstransfer abgewickelt worden sein. Wenn man das einbaut, kann man die Software später mal (wenn man selber als großer, zuverlässiger Rechenknoten etabliert ist) rausgeben und quelloffen machen und sich trotzdem ne goldene Nase verdienen. Aber vermutlich klappt das nicht, denn man müsste dafür selber große Rechenkapazitäten haben, die man sich wohl bei Amazon mieten würde. Und dann würde jeder Hinz und Kunz bei Amazon mieten, oder die Leute würden immer direkt ihr Zeug bei Amazon aufsetzen. Andererseits: Normale Enduser wollen einfach nur mit dem Ding spielen und beeindruckende Dinge sehen. Dafür würden die auch Geld auf den Tisch legen...

Man sollte es so machen, dass die Eigenschaften, die wir pro Teilchen tracken, solche wie diese hier sind:

	Position
	Geschwindigkeit
	Masse
	Graviation
	Starke Kernkraft
	Schwache Kernkraft
	Elektromagnetische Wechselwirkung
	Teilchentyp (Elektron, Proton, Neutron, Wasserstoffatom, Alkoholmolekül)
	Chemische Reaktionen
	Nukleare Reaktionen
	Ko-/Adhesion von Molekülen
	
Es muss also Plugins geben, die spezifizieren, welche Datenfelder ein Teilchen hat und welche globalen Daten sie brauchen. Sie spezifizieren die durchzuführende Berechnung  (die den Zustand des Universums von einem Zeitpunkt zum nächsten updated) als Kernel, die pro Teilchen in Stufen ausgeführt werden. Pro Stufe werden also alle Teilchen mit dem Kernel geupdated, dann wird der Speicher für alle beteiligten Threads geflusht (barrier, damit alle dasselbe sehen) und dann wird die nächste Stufe gegeben. Dieses Verfahren eignet sich für alle oben beschriebenen Wechselwirkungen und erlaubt es, generische Propagatoren für die GPU, CPU und Netwerk zu bauen, die alle miteinander betrieben werden können. Die Kernel müssen so geschrieben sein, dass man sie auf verschiedene Arten verteilen kann: Entweder man partitioniert Teilchen und lässt eine Wechselwirkung von mehreren Knoten berechnen, oder man partitioniert Wechselwirkungen und lässt ein Teilchen von mehreren Knoten berechnen.
Es muss während der Simulation möglich sein, Propagatoren hinzuzufügen oder abzuziehen. Idealerweise ist die Kanalstruktur zwischen den Threads nicht zentralisiert, sondern peer-to-peer, d.h. Tim kann einen Knoten aufsetzen, der alle seine Rechner zu Hause bündelt und die gebündelten Daten mit Toms Zentralrechner teilt, der hin und wieder mal noch seinen Server mit einklinkt.  Die Simulation soll das Ergebnis eines jeden Zeitschritts in einen (blockierenden) Buffer von Zuständen schreiben. Aus dem mag man die Zustände dann herausnehmen, um daraus Bilder und Videos zu berechnen, sie auf die Platte zu speichern, oder im Netz weiter zu versenden. Das ganze Netz kann sich selbst load-balancen.

Schön wäre es, wenn die Zeitschritte eine adaptive Länge hätten (wenn nicht viel mit den Teilchen abgeht, kann das Intervall ja ruhig ein bischen länger sein). Außerdem sollte es möglich sein, zwischen zwei gegebenen Zeitschritten einen Zwischenschritt zu interpolieren.


Die Berechnung soll auf Knoten verteilt werden. Knoten können sein: CPU's, GPU's, entfernte Rechner. Außerdem gibt es Meta-Knoten, die all ihre Arbeit einfach auf andere Knoten verteilen. Man kann es auch so sehen, dass CPU's und einzelne Vektor-Lanes einer GPU Blätter im DAG sind, während Meta-Knoten innere Knoten im DAG sind. Der Nutzer soll die Knoten und ihre Verbindungen definieren. Er baut dabei einen DAG. Es gibt dabei zwei Kantentypen:  

Es gibt zwei Arten von Kanten: a ==> b heißt "Knoten a darf Aufträge für die aktuelle Iteration an Knoten b geben". 
							   b --> a heißt "Knoten a darf sich Informationen der vorherigen Iteration von Knoten b beschaffen".
							   
Ausgewählte Wurzelknoten (meistens genau einer) werden vom Nutzer bedient. Von ihnen gehen die Aufträge aus. Der DAG ist getaktet: Ein Knoten erhält einen Berechnungsauftrag. Entweder teilt er diesen auf seine ==> Nachfolger auf, oder er rechnet selber. Irgendwann braucht er Informationen über den Zwischenzustand der restlichen Knoten. Dann holt er sich (verteilt über seine --> Vorgänger) Informationen über die Auftragsergebnisse des restlichen Netzes. Erst wenn er die alle hat, fährt er mit dem Auftrag fort. Falls dem Knoten selbst eine solche Anfrage gestellt wird, versucht er sie (falls er selber ein Arbeiter ist) aus seinem Auftragszwischenergebnis zu entnehmen, oder er fragt bei seinen --> Vorgängern an. Das wiederholt sich, bis der Auftrag des Knotens abgeschlossen ist.
Man beachte, dass während einer Iteration Anfragen für eine vorangegangen Iteration immer über die --> Kanten laufen, während Anfragen für die aktuelle Iteration über ==> Kanten empfangen werden. Es darf keine ==> Zyklen geben, --> Zyklen sind erlaubt. Wenn a ==> b und a --> b, kann es nicht sein, dass a's Auftrag an b einfach von b durch Abfragen der Informationen von a beantwortet wird (was zu einem Deadlock führen würde): a fragt nach Daten für die aktuelle Iteration, wohingegen b von a nur Daten über die vorherige Iteration anfragen kann. Ein Knoten gibt also über eine --> Kante nur Informationen zu einem Auftrag weiter, wenn ihm dessen Beendigung über ==> Kanten gemeldet wurde.

Die Kanten werden wie gesagt vom Nutzer definiert, aber welche davon ein Knoten wie stark beansprucht, um Aufträge zu verteilen und Statusupdates zu sammeln, ist Sache des Knotens (der durch Zeitmessung Load-Balancing macht).
Man beachte, dass das Auftragsrouting mehr oder minder statisch ist: Ein Knoten geht immer davon aus, dass sein nächster Auftrag die selben Teilchen betreffen wird und fährt ohne weiteres abwarten sofort mit diesem fort, sobald der aktuelle abgeschlossen und über die --> Kanten die nötigen Informationen über den Zustand abgeholt sind. Das Ausbleiben von Aufträgen muss vom ==> Vorgänger aktiv signalisiert werden (z.B. weil dieser anders balancen will).
Einfache Daumenregeln: Über --> Kanten liefert ein Knoten nie Daten aus, für deren Berechnung er selber einen Auftrag vergeben hat, der noch nicht abgeschlossen wurde.
		Außerdem gilt: Eine Anforderung von Daten über eine --> Kante führt nie dazu, dass der Knoten etwas berechnet, oder einen Auftrag vergibt! Er wird höchstens feststellen, dass er die angeforderten Daten nicht hat und sie über seine --> Kanten anfordern.


Schön wäre es außerdem, wenn man verteilte Architektur als separate Komponente wiederverwenden könnte (z.B. um einen verteilten Raytracer zu implementieren...)

Um schnell was nettes zu haben, mit dem man ein bischen spielen kann, muss ich 
wohl davon ausgehen, dass alle Integratoren in den Knoten schon vorhanden 
sind. Richtig perfekt wird die Sache aber erst, wenn ich eine Particle-DSL 
erfinde, die den Übergang eines Teilchens von einem Zeitschritt zum nächsten 
beschreibt. Damit wäre für den Nutzer sehr viel hässliches Zeug weg 
abstrahiert und man könnte beliebige Integratoren sehr schnell einbauen und 
übers Netzwerk an fremde Knoten senden (die allerdings vermutlich LLVM und 
nvcc installiert haben müssen, falls sie die DSL *schnell* ausführen wollen). 
Ich bräuchte dann einen Compiler für die DSL und vermutlich auch eine Data 
Dependence Analyse (für die Snchronisation), aber das erscheint mir die Sache 
wert...

Phänomene, die man simulieren können soll:

Newton'sche Gravitation
chemische Rekationen
Explosionen
Raketentriebwerke
Nukleare Kettenreaktionen
Phasen im Leben eines Sterns
Glaxien samt schwarzer Löcher und Sterne mit Planeten und Monden
Wasserplanschen
Kuchenbacken
Feuer
Fischschwärme
Ameisenhaufen
Räuber-Beute-Modelle (Haie und Fische...)
Zelluläre Automaten
Festkörperkollisionen mit Deformierung
Aeordynamik
Biologische Zellen (z.B. Lipidmembran usw...)
Biologisches Wachstum nach DNA
Gehirnwachstum
Soziale Netzwerke (Entwicklung und Interaktion der Individuen einer Gesellschaft)
Wetter
Prozesse in einem Organsimus (z.B. Verteilung von Sauerstoff übers Blut, Stoffwechsel, Vireninfektionen, Medikamentengabe, ...)
Ausbreitung von Pflanzen und Tieren.


============= Ältere Gedanken, die unter obigen Eindrücken nochmal überprüft werden müssen: ==============



Entstehung kleiner Universen simulieren: Ich will beobachten, wie kleine Partikel sich unter ihrer eigenen Schwerkraft zu komplexen Systeme zusammenziehen. Dazu soll man mit der Maus Partikel auf dem Bildschirm verteilen und ihnen mit einem "Schubs" einen Impuls verleihen. Partikel haben die Eigenschaften Ort, Geschwindigkeit, Protonenzahl, Neutronenzahl, Elektronenzahl und eventuell auch Name und Farbe, sowie Größe und Masse (welche sich allerdings aus den anderen Eigenschaften berechnen ließe).
Der User soll per ComboBox-Menü auswählen können, welche Partikel (Wasserstoff-Atome, Sauerstoff-Atome, Elektronen, Photonen, ...) er gerade erzeugt. Außerdem soll er die vier Grundkräfte der Physik jeweils aktivieren oder deaktivieren können, um dann zu beobachten, wie die Teilchen in seinem Universum sich verhalten. Da könnte man dann solche Sachen wie die Entstehung von stabilen Umlaufbahnen, von Sternen und vielleicht sogar das Fließen von Strom und das Emittieren von Lich (Raytracer!) beobachten... Das ganze sollte, um anschaulich zu bleiben und nicht zu rechenhungrig zu werden, in 2D stattfinden! Allerdings könnte man auch nach der Dimensionalität parametrisieren. Es bietet sich aber auch an, das Modell grundsätzlich in 3D zu basteln, denn das ist sehr *universal*...Die Kräfte
ließen sich vielleicht mit Integratoren ähnlich denen in Numitras simulieren...
Die Stärken der Kräfte sollen regelbar sein, die Eigenschaften von Protonen und Neutronen (z.B. Masse und Größe) dagegegen nicht!
Reaktionen zwischen Partikeln sind definiert durch probabilistische Inferenzregeln: Wenn bestimmte Bedingungen zwischen zwei Teilchen herrschen, werden diese durch das Ergebnis der Regel ersetzt (induzierte Kernspaltung, spontane Kernspaltung, Kernfusion, Weitergabe von Elektronen, Emission von Lichtquanten mit bestimmter Frequenz...).
Siehe http://de.wikipedia.org/wiki/Kernspaltung und http://de.wikipedia.org/wiki/Kernfusion, sowie den Wikipedia-Artikel über Bindungsenergie!
Um später noch Dinge wie Photonen einfügen zu können, sollte ich wahrscheinlich viel mit Interfaces arbeiten. Überhaupt empfiehlt sich ein wohlüberlegtes Design, da ja absehbar ist, was als künftige Erweiterungen eventuell in Betracht kommt...
Das AMG könnte ein solches Tool ja mal in seinen Physik-Unterricht einbauen...
Als Sprache empfiehlt sich aus Performanzgründen C++ mit Qt, aber einfacher umzusetzen wäre es wahrscheinlich in C# mit Windows Forms (wobei das doch recht langsam und schlecht zu Linux kompatibel ist). Es empfiehlt sich, Hardwarebeschleunigung zu nutzen. Für einen Raytracer in 3D könnte man das ganze auch so implementieren, dass nur dann ein Pixel gesetzt wird, wenn es von einem Photon getroffen wurde...
Am nettesten wäre es natürlich, das in Programma zu schreiben, wofür man aber erst mal einen Programma-Compiler und eine grundlegende GUI-Bibliothek bräuchte...
Mir scheint, das folgendes gilt:
1. Alle Vorhersagen der speziellen Relativitätstheorie ergeben sich daraus, dass jeder Beobachter im Universum für das ihn erreichende Licht immer die Geschwindigkeit c misst!
2. Alle Vorhersagen der allgemeinen Relativitätstheorie ergeben sich daraus, dass Masse die Raumzeit "krümmt" (was sich für uns dreidimensionale Ameisen nicht zeigt), weshalb die Flugbahnen von Objekten um sie herum durch einen "Massetrog" deformiert werden.
   Deshalb wird z.B. Licht auch durch Gravitation beeinflusst. Vielleicht lässt sich mit E = mc² aber auch dem Licht eine Masse zuschreiben, aus der sich mit der Gravitationsformel die laut Einstein vorhergesagte Ablenkung ergibt?
   Man beacht den dynamischen Massezuwachs (m(v)) !
Wie sich allerdings herausgestellt hat, ist, wie der Name es bereits vermuten lässt, die spezielle Relativitätstheorie in der allgemeinen enthalten!
Es ist anzustreben, dass ich mich bei der Implementierung des Basissystems nicht auf eine bestimmte Theorie von Raum und Zeit festlege!
Besser wäre es, das System so auszulegen, dass man auch bezüglich Quantenmechanik und Stringtheorien damit experimentieren kann! (Sofern das machbar ist).
Dann brauche ich mich selber nicht um die Relativitätstheorie und die Quantenelktrodynamik zu kümmern, sofern ich das nicht will!
Zu diesem Zweck sollte man höchstens fordern, dass Partikel eine dreidimensionale Position bereitstellen, damit man sie darstellen kann!
(Schließlich wollen wir uns nicht darauf verlassen, dass der Nutzer seinen Rechner tatsächlich mit der Simulation von Photonen beladen will...)
Es ist zu untersuchen, inwiefern dann die Konzepte Kraft und Beschleunigung verallgemeinert werden müssen und ob man immer noch RungeKutta-Integratoren einsetzen kann!
Die Sache mit den durch Inferenzregeln definierten Partikelreaktionen jedenfalls, soll unbedingt erhalten bleiben, damit der Nutzer von unnötigen Details abstrahieren kann!
Ich habe Grund zu der Hoffnung, die Effekte der speziellen Relativitätstheorie ließen sich vollständig modellieren, wenn man im Modell nicht von einer beschränkten Lichtgeschwindigkeit ausgeht 
	(d.h. wenn ein Atom, dass mit halber Lichtgeschwindigkeit ein Elektron mit Lichtgeschwindigkeit aussendet, soll dieses im Modell 1,5-fache Lichtgeschwindigkeit haben!)
	und in der Darstellung des Simulationsablaufs die Lorentztransformtion auf die Modell-Koordinaten loslässt 
	(Wenn sich besagtes Atom relativ zum Beobachter bewegt, kann man seine Raumzeitkoordinaten für Zustand der Simulation  mittels Lorenztransformation entlang des aktuellen Geschwindigkeitsvektors transformieren. Gleiches gilt auch für das Elektron, so dass beide Teilchen langsamer werden, wobei das Elektorn in der Darstellung dann genau mit Lichtgeschwindigkeit erscheint!)
	Man beachte hierfür vor allem den ersten Abschnitt des Wikipedia-Artikels zur Lorenztransformation!
Wenn sich die spezielle Relativitätstheorie so implementieren lässt, muss sie im Modell gar nicht beachtet werden :)
Es bleibt die allgemeine. Schön wäre es, wenn man gemäß E(v) = m(v) * c^2 allen Objekten, auch Lichtteilchen eine Masse zuschreiben und damit ihre Anziehung modellieren könnte.
Dann könnte man sich vielleicht das scheinbar komplexe Krümmen der Raumzeit ersparen. Da Licht dann also auf Massen zubeschleunigt wird, sieht der Beobachter aufgrund der dadurch erhöhten Geschwindigkeit des Lichts gemäß der speziellen Relativitätstheorie tatsächlich eine Krümmung.
Wenn sich dieses tatsächlich so implementieren ließe (wofür es aber unter http://de.wikipedia.org/wiki/Masse_%28Physik%29#Spezielle_Relativit.C3.A4tstheorie Gegenrede gibt), bräuchte das Basissystem die Relativitätstheorie nicht zu betrachten :) Das würde alles über Plugins erledigt...
(Und eventuell müsste die GUI da einiges berücksichtigen, vor allem was Zeitdilatation und Längenkontraktion angeht, aber auch das würde man in Plugins kapseln).
Überhaupt wäre ein größtmögliches Maß an Modularisierung wünschenswert, damit ganz verschiedene Modelle der Wirklichkeit, also z.B. auch die Äthertheorie und anderes Zeug betrachtet werden können!
Die Krönung wäre, das System so allgemeingültig und flexibel zu halten, dass sich damit eines Tages auch eine (funktionierende) String-Theorie simulieren ließe...
Diese Flexibilität ist wahrscheinlich, denn wahrscheinlich weder Zeit noch Lust, mich mit den Feldgleichungen der ART und Unwägbarkeiten der Quantenwelt auseinanderzusetzen
Wie bauen wir also die Simulation?
Sagen wir mal: Zentral ist die Klasse Particle.
			   Ein Simulationsszustand besteht aus einem Zeitpunkt t und der Information über die Zustände im Zeitpunkt t existenten Teilchen.
			   Die Simulation-Klasse propagiert vom aktuellen Zustand immer zum nächsten, mit konfigurierbarer, eventuell adaptiver Zeitschrittweite, bis man sie anweist, aufzuhören (siehe Skizze zur Simulation-Klasse)


Es fragt sich, welche Klasse die Integration eines Teilchens durchführt. Alternativen:
	1. Die Simulation hält den Integrator und integriert damit alle Teilchen. Problem: Dafür muss die Simulation davon ausgehen können, dass jedes Teilchen gewisse Eigenschaften, wie Ort und Geschwindigkeit (Achtung, Heisenberg!) besitzt.
	2. Jede Kraft hat einen Integrator. Problem: Das ist unsinnig, denn um zu integrieren, benötigt der Integrator die Summe aller Kraftvektoren, die das Teilchen beschleunigen. Die Kräfte müssten also untereinander kommunizieren (aufwändig zu coden, fehleranfällig, langsam)
	3. Jedes Teilchen ist sein eigener Integrator. Das kommt mir am allerschönsten vor. Die Klasse Particle sieht dann so aus:
		abstract class Particle {
			abstract ParticleState integrate(SimState s); // berechnet den nächsten Teilchenzustand
			abstract ParticleState interpolate (ParticleState past, time now, ParticleState future); //"Mittlet" zwei zeitlich diskrete Teilchenzustände
			abstract void draw(Graphics g); //Zeichnet das Teilchen. (Stattdessen könnte auch die Übermittlung der Position verlangt werden, damit ein bewegter Beobachter die SRT berücksichtigen kann)
		}
		
		Die Methode integrate übernimmt dabei nicht nur die Positionspropagation, sondern könnte z.B. auch Reaktionen mit anderen Teilchen implementieren.
		Ob ein anderes Teilchen mit dem aktuellen wechselwirkt, hängt natürlich auch von dessen Parametern und vor allem seinem Typ ab. Z.B. wird ein Wasserstoffatom gravitativ nur mit den Teilchen wechselwirken, die das Interface IMassy implementieren...
		
		Der ParticleState allerdings ist etwas unschön: Da die Simulation ja (oft) nicht in Echtzeit läuft, müssen die Stützpunkte protokolliert werden. Es ist dabei zuerst einmal zu untersuchen, welche Funktionalität in der Particle- und welche in der ParticleState-Klasse vorzuhalten ist...
		Möglicherweise erweist es sich aber auch als nötig, Particle-Instanzen wirklich kopieren zu können... Schließlich will man eine Simulation, die man gerade abspielt, vielleicht ja auch anhalten und dann modifizieren... Andererseits könnte es sich auch als günstig erweisen, im Simulationsprotokoll möglichst viele Daten der Particle-Klasse zu verwerfen, um effizient damit arbeiten zu können... Dann wäre vielleicht Konstruktoren Particle(ParticleState s) und ParticleState(Particle p) nötig... (wobei mir das ein wenig fragwürdig vorkommt, da es einen Isomorphismus auf den beiden induzieren müsste...)
	
	
	
Und hier ist der Plan:

	Wir entwickeln in C++.
	Der Kern des Programms ist eine Bibliothek namens Universe, die die gesamte Simulationslogik bereitstellt. Darauf aufbauend soll es verschiedene GUI-Frontends geben.
	Für den Anfang wollen wir nur ein einfaches 2D-Frontend, damit man schnell was ansehnliches bekommt, ohne sich zu sehr mit OpenGL rumschlagen zu müssen.
	Die Kernbibliothek muss sich für folgende Anwendungsszenarien eignen:
		Simulation kleiner Systeme (z.B. 1000 Partikel) in Echtzeit in 2D/3D. Der Nutzer soll mit dem System interagieren können.
		Simulation gigantischer Systeme (bis zu 1 Milliarde Partikel). Die Simulation soll sich vom Benutzer im Ressourcenverbrauch beschränken, sowie anhalten und fortsetzen lassen, damit eine Berechnung über größere Zeiträume möglich ist. Idealerweise ließe sich die Simulation nicht nur auf verschiedene Prozessoren, sondern sogar auf räumlich und zeitlich getrennte Computer verteilen!
		Darstellung bereits fertig vorberechneter Simulationen in Echtzeit in 2D/3D.
		Ausgabe einer Videodatei aus dem simulierten Universum.
		
	Mögliche Darstellungsarten:
		2D
		3D mit schwacher Beleuchtung und Matrizengeometrie
		3D per Raytracing.
	
	
	
	
Wir müssen folgende Entitäten betrachten:

	Partikel
		Partikel besitzen gewisse Eigenschaften, aber nicht alle Partikel besitzen alle Eigenschaften (z.B. hat nicht jeder Partikel eine Farbe oder eine Frequenz)
		Wasserstoffatom, Eisenatom, Wassermolekül, Proton, Neutron, Elektron, Photon, ...
	Kräfte
		Geben an, wie ein gegebener Partikel zum gegebenen Zeitpunkt zu beschleunigen ist. Dazu brauchen sie Zugriff auf die gesamte Simulation.
		Beispiele: Gravitation, Coulombkraft, Kernkräfte, Lorentzkraft
		Möglicherweise sollte das Modell der Simulation gar keine Kräfte kennen... Viel eher wären diese wohl den entsprechenden Integratoren unterzuordnen. Aber Achtung: Die Berechnung von Kräften (wie z.B. Gravitation und Coulombkraft) ist sehr aufwändig, weshalb man dafür den Barnes-Hut-Algorithmus verwendet.
		Meine Überlegungen zeigen, dass es Sinn macht, die Kräfte im Kernsystem komplett außen vor zu lassen. Vielmehr werden sie durch Integratoren eingeführt und müssen nur diesen bekannt sein. Eine Instanz des Barnes-Hut-Algorithmus können sich die Integratoren mit diversen technischen Tricks sehr leicht teilen.
	Integratoren
		Sie erhalten in jeden Zeitschritt die Gelegenheit, die Eigenschaften der Teilchen zu verändern, bzw. sie durch neue Teilchen zu ersetzen.
		Beispiele: RungeKutta für die Bewegung, Spaltung/Fusion für Atomkerne, Current für die Abgabe/Aufnahme von Elektronen.
		Integratoren sind zentrale Bestandteile der Simulation. Obwohl sie Informationen an die Partikel anheften können, gibt es z.B. nur einen einzige Instanz des RungeKutta-Integrators, die für jedes Teilchen einmal pro Zeitschritt aufgerufen wird.
		Eventuell könnte es nützlich sein, Integratoren nicht dazu zu zwingen, für jeden Zeitschritt tatsächlich eine Stützstelle anzulegen...
		Sollte ein Integrator (so wie die RungeKutta-Integratoren) zusätzliche Unterstützstellen benötigen, so ist das sein Problem. Er muss die Daten dafür in den vorgesehenen Feldern der Partikelobjekte speichern.
		

	Die Simulation muss also ihre Partikel und die Integratoren kennen, die beide auf die soeben beschriebene Weise arbeiten.

Einen abgeschmackten Prototypen, der um beliebige Partikel, Kräfte und Reaktionen erweiterbar ist, können wir mir aufgrund dieser Überlegungen bereits bauen. Es bleiben zwei Probleme:
1. In welcher Form stellen wir den Output der Simulation bereit, um ihn geeignet weiterzuverarbeiten.
	Antwort: Das Simulation-Objekt enthält immer nur die aktuelle Stützstelle. Die Daten werden sofort nach ihrer Berechnung in einen "Abnehmer" gedumpt. Solche Abnehmer könnten ein Wrapper für Festplatten oder ein Speicherstream sein, aber auch ein Display-Objekt, dass die Daten auf dem Bildschirm darstellt, sowie ein Video-Encoder, der ein Video der Simulation erstellt. Auf diese Weise sind nur dann große Speicherressourcen erforderlich, wenn die Simulation in ihrer Gesamtheit gespeichert werden soll. und z.B. nicht für die Erstellung eines Videos. Idealerweise sollten aber sogar mehrere Dump-Objekte bedient werden! Die Dump-Objekte entscheiden selbst, was sie mit der ihnen übergebenen Stützstelle anfangen wollen und sollten eigene Agenten sein!
	
2. Wie werden wir durch verteiltes Rechnen des großen Aufwandes Herr?
	Es soll möglich sein die Simulation auf mehrere Berechnungseinheiten zu verteilen. Ein ideales Szenario wäre: Lokal starte ich die Simulation auf meinem Rechner und erlaube ihr, 3 meiner 4 Prozessorkerne voll auszulasten, während der vierte nur entsprechend dessen sonstiger Last genutzt werden soll. Dann aktiviere ich das Lauschen am Netzwerk. Andere Rechner sollen sich darüber nun in die Simulation einklinken können und meinem Rechner Last abnehmen bzw. die Berechnung beschleunigen. Nach 8 Stunden entscheide ich, ins Bett zu gehen und fahre meinen Rechner herunter. Die anderen eingeklinkten merken das und rechnen munter weiter. Am Morgen fahre ich meinen Rechner wieder hoch und setze die Simulation fort, helfe also den anderen (von denen einige sich ebenfalls deaktiviert haben und zu denen neue dazugestoßen sind) wieder. Ich lasse auch meinen Zweitrechner mit einsteigen. Da der aber recht langsam ist, antwortet er zunächst zu langsam, weswegen mein Rechner seine Kommunikation zunächst an andere 
weiterleitet.
	Dann stürzt mein Zweitrechner wegen Überhitzung ohne Vorwarnung ab. Die anderen nehmen das zur Kenntnis und setzen die Simulation unbeirrt fort.
	Auf meinem Erstrechner erlaube ich nur noch die Nutzung eines einzigen Kerns, weil ich mir den Simulationsstatus mal ansehen will. Mit einem Live-Viewer kann ich - ähnlich wie bei einem YouTube-Video im bereits vorberechneten Teil der Simulation umherspringen und mich umschauen. Da mir gefällt was ich sehe, setze ich die beiden Kerne wieder aktiv und starte erneut meinen Zweitrecher. Diesmal soll er sich jedoch nicht an der Berechnung beteiligen, sondern, ein Video rendern. So geht das einige Tage weiter. Dann schaue ich mich erneut um und sehe, dass das System in einen stabilen Zustand übergegangen ist. Ich beende also lokal die Simulation. Meine Mitstreiter entscheiden sich jedoch, weiter zu rechnen. Auch mein Erstrechner beschäftigt von nun an 2 seiner Kerne damit, das Video zu rendern. Nach einem Tag erhalte ich die Meldung, dass beide Rechner fertig sind, und dass das Video nun bereitsteht.

	Diese Funktionalitäten erreichen wir folgendermaßen:
		Es ist davon auszugehen, dass zur Berechnung eines neuen Zustandes der alte Zustand in seiner Gänze vorliegen muss. Wir können also nicht die simulierte Zeit parallelisieren, sondern müssen zwischen den Zeitpunkten komplett synchron sein: Mehrere Agenten teilen sich den Zeitschritt und berechnen ihn aufgrund der Daten des vorrangegangenen Zeitschritts parallel. Erst wenn alle Agenten fertig sind, wird der nächste Zeitschritt in Angriff genommen.
		Ein Zeitschritt ist der Zeitraum zwischen zwei Stützstellen.
		
		Eine Simulation wird auf zwei verschiedene Arten parallelisiert: Zum einen stehen der Simulation mehrere Recheneinheiten zur Verfügung, die sich einen gemeinsamen Speicher teilen. Zum anderen ist das Simulationsobjekt Teil eines gerichteten Graphen von Simulationsobjekten, die sich ihren Speicher eben nicht teilen, sondern untereinander kommunizieren müssen.
		Das Simulationsobjekt hat Vorgänger (Knoten, die Daten liefern) und Nachfolger (Knoten, die Daten erhalten). In der Berechnung eines Zeitschrittes setzt das SO die ihm zu Verfügung stehenden Rechenkerne ein, um alle Nachfolger zu bedienen: Die Nachfolger geben alle bekannt, über welche Partikelpakete sie welche Informationen geliefert haben möchten. Das SO liefert diese Informationen, sobald es sie hat und zwar entweder, weil ein Vorgänger sie geliefert hat, oder weil es sie selbst berechnet hat. Diese Bekanntgabe oder "Bestellung" nennen wir Scope eines Berechnungsknotens. Er umfasst den Teilchenbereich und die Art der vom Knoten behandelten Teilcheninformationen (siehe auch weiter unten: Nicht alle Simulationsknoten besitzen alle Integratoren!), sowie die Zeitschritt-ID, für die die Informationen benötigt werden.
		Dabei werden die lokalen Kerne so gründlich wie möglich bzw. erlaubt ausgenutzt und es wird aufgrund einer dynamischen Performance-Schätzung der Rest der Daten von den Vorgängern verlangt. Das Berechnungsgrid verteilt seine Rechenlast auf diese Weise automatisch und dezentral.
		
		Die Integratoren werden also nur für die Teilchen aufgerufen, für die sich das SO im Grid zuständig fühlt. Trotzdem braucht jeder Knoten potenziell Informationen über *alle* Partikel des vergangenen Zeitschrittes. Die SO geben diese Informationen über's Netzwerk weiter. Diese Weitergabe muss so beschaffen sein, dass nur die nötigen Informationen weitergegeben werden: Beispielsweise könnte ein Knoten nur für die Gravitation einer Hälfte der Teilchen zuständig sein, nicht aber für ihre Kernreaktionen. Deshalb sollten für diesen Knoten auch nur die Informationen des Gravitationsintegrators übertragen werden und nicht die Informationen für den Kernreaktionsintegrator.
		Hieran wird deutlich: Obwohl alle Simulationsknoten alle Teilchen behandeln müssen, haben sie nicht alle die selben Integratoren.
		
		
************ Aktueller Stand *************

Mit obenstehendem (ab "Wir müssen folgende Entitäten betrachten") ist die Problemlösung wahrscheinlich vollständig umrissen. Diese muss nun weiter ausgearbeitet werden, damit quasi eine dokumentierte Leerimplementierung der Kernkomponente in C++ entstehen kann.
		